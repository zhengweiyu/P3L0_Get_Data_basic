{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习: 运营商列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FL', 'AS', 'AA', 'MQ', '5Y', 'DL', 'EV', 'F9', 'HA', 'B6', 'OO', 'WN', 'NK', 'US', 'UA', 'VX']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        select = soup.find(id='CarrierList')\n",
    "        for option in select.find_all('option'):\n",
    "            if \"All\" not in option.get_text():\n",
    "                data.append(option.get('value')) \n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATL', 'BWI', 'BOS', 'CLT', 'MDW', 'ORD', 'DFW', 'DEN', 'DTW', 'FLL', 'IAH', 'LAS', 'LAX', 'ABR', 'ABI']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        # print(soup.prettify())\n",
    "        select = soup.find(id='AirportList')\n",
    "        for option in select.find_all('option'):\n",
    "            if \"All\" not in option.get_text():\n",
    "                data.append(option.get('value')) \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2env]",
   "language": "python",
   "name": "conda-env-py2env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
